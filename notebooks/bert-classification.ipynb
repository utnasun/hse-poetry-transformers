{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.chitalnya.ru/work/3180020/',\n",
       " 'rating': '29',\n",
       " 'views': 33,\n",
       " 'output_text': 'Люблю ли осень? Ты спроси у ели -\\nОна в колючках и не ждёт тепла...\\nЕй хрен один: повсюду лишь метели,\\nКапель ли хнычет ночью со стекла.\\n\\nБагряный лист, ли, кружится прощаясь,\\nЛи фонаря торчащего кадык,\\nНа низком небе пятнами касаясь\\nК луне общаясь, притулился встык.\\n\\nЛюблю ли осень? Ты спроси у ветра.\\nС ним всё равно не сладить в парусах.\\nПо осени ли, по весне для шторма,\\nСпускают их, на реи намотав.\\n\\nПожухлых трав не кошенные дали,\\nЧьих жалких тел охапками нажнут.\\nСпроси у них, о том они мечтали\\nВесной, тогда!? Теперь их просто жгут.\\n\\nПо осени, когда седеют чувства,\\nЗастыли ветви рук на деревах -\\nМне хочется упасть на дно колодца,\\nЧто бы не видеть слёзы на глазах.',\n",
       " 'genre': 'лирика'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "with open('data/poetry_data_train.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = [entry['output_text'] for entry in data if float(entry['rating']) > 0 and entry['views'] >= 50]\n",
    "ratings = [float(entry['rating']) for entry in data if float(entry['rating']) > 0 and entry['views'] >= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = pd.qcut(ratings, 10, labels=False)  # Разделяет рейтинги на 10 категорий\n",
    "ratings = ratings.tolist()\n",
    "\n",
    "np.unique(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemRatingDataset(Dataset):\n",
    "    def __init__(self, poems, ratings, tokenizer):\n",
    "        self.poems = poems\n",
    "        self.ratings = ratings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        poem = self.poems[idx]\n",
    "        rating = self.ratings[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            poem,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(rating, dtype=torch.long)  # Изменение на целочисленную метку\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "dataset = PoemRatingDataset(poems, ratings, tokenizer)\n",
    "\n",
    "# Train-validation split\n",
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=10).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2237/2237 [12:23<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Training loss: 2.256960899796825\n",
      "Validation loss: 2.260660559444104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2237/2237 [12:23<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4, Training loss: 2.258260872782902\n",
      "Validation loss: 2.255840838965723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2237/2237 [12:23<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4, Training loss: 2.2579921418447073\n",
      "Validation loss: 2.2549050318992743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2237/2237 [12:23<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4, Training loss: 2.256723910450243\n",
      "Validation loss: 2.25733257956424\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training loss: {total_train_loss / len(train_dataloader)}\")\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation loss: {total_val_loss / len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert_classification/tokenizer/tokenizer_config.json',\n",
       " 'bert_classification/tokenizer/special_tokens_map.json',\n",
       " 'bert_classification/tokenizer/vocab.txt',\n",
       " 'bert_classification/tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), f'bert_classification/classification_poem_rating_model_filtered.pth')\n",
    "tokenizer.save_pretrained(f'bert_classification/tokenizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчет NDCG на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.chitalnya.ru/work/2999707/',\n",
       " 'rating': '0',\n",
       " 'views': 15,\n",
       " 'output_text': 'Нынче головы четыре у дракона,\\nПожиратель президентов он мастак.\\nНа него управы нет, и нет закона,\\nВ споре с ним же попадёт любой впросак.\\n\\nУ него в кармане сотни триллионов.\\nДля него богач - обычный нищеброд.\\nНету больше никаких на свете тронов,\\nНи во что не ставит он любой народ.\\n\\nОн прикажет слугам - выпилить любого\\nС интернета, те его прогонят прочь,\\nИ никто не пикнет, не промолвит слова,\\nНекому бедняге на земле помочь.\\n\\nИ всё потому, что присягнули змею,\\nОтступив от Бога, скорбен наш удел.\\nСлужат люди молча мерзкому пигмею,\\nПоголовно, будто вовсе оборзев.\\n\\nНынче люди лживы, врут напраполую.\\nОн сегодня баба, завтра же мужик.\\nГрабят, убивают, даже мать родную\\nМогут погубить, коль сам дракон велит.\\n\\nВремена драконьи - времена инферно.\\nКовид пандемия - следствие его,\\nОбезумевши, спешит планету ввергнуть\\nВ бездну - так уже желает бошинство.',\n",
       " 'genre': 'лирика'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "with open('data/poetry_data_test.json', 'r', encoding='utf-8') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "\n",
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 714.29 MB. The target location /root/.cache/huggingface/hub/models--DeepPavlov--rubert-base-cased/blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "path = 'bert_classification/classification_poem_rating_model_filtered.pth'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=10)\n",
    "model.load_state_dict(torch.load(path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = [entry['output_text'] for entry in test_data]\n",
    "ratings = [float(entry['rating']) for entry in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "test_dataset = PoemRatingDataset(poems, ratings, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predicted_ratings = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        predictions.extend(predicted_ratings)\n",
    "\n",
    "# Теперь у вас есть список предсказанных рейтингов\n",
    "predictions = np.array(predictions)\n",
    "true_ratings = np.array(ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_dcg(scores, k=10):\n",
    "    \"\"\"Calculate DCG given a list of relevance scores, limited to top k positions.\"\"\"\n",
    "    gains = scores / np.log2(np.arange(2, scores.size + 2))\n",
    "    return np.sum(gains[:k])\n",
    "\n",
    "def ndcg_score_large_dataset(y_true, y_pred_classes, k=10):\n",
    "    ndcg_scores = []\n",
    "    for true_ratings, pred_classes in zip(y_true, y_pred_classes):\n",
    "        sorted_pred_indices = np.argsort(pred_classes)[::-1]\n",
    "        sorted_true_indices = np.argsort(true_ratings)[::-1]\n",
    "\n",
    "        pred_ordered_true_ratings = true_ratings[sorted_pred_indices]\n",
    "        \n",
    "        dcg = calculate_dcg(pred_ordered_true_ratings, k)\n",
    "        idcg = calculate_dcg(true_ratings[sorted_true_indices], k)\n",
    "\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "\n",
    "ndcg = ndcg_score_large_dataset(true_ratings, predictions, k=10)\n",
    "print(f\"Normalized Discounted Cumulative Gain (NDCG): {ndcg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
